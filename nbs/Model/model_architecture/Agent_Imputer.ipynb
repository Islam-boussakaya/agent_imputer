{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d9d9f5-501b-4127-864f-1241e3ffc812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.agent_imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfee6c-350e-4348-8fa9-8b4fcee6dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42550ad5-a02e-44b9-a406-7ab6aea266c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import model_architecture.Time_LSTM_Module as TimeLSTM\n",
    "import model_architecture.GNN_Module as GCN\n",
    "import pytorch_lightning as pl\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bbb8f-aff2-497d-ae25-ecbe7a93557c",
   "metadata": {},
   "source": [
    "# Agent Imputer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5dcb69-0c24-47a9-baf1-bd7899008e52",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../img/model_schema.png\" />\n",
    "<figcaption>Fig.1 Agent Imputer model architecture.</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96fb1f3-1b4e-49ca-8b37-c80cf2108ca6",
   "metadata": {},
   "source": [
    "$N$ = 22 agent. $B$ = Batch size. $L$ = sequence length. $I$ = features number. $H_{1}$ = 100 hidden size layer. $H_{2}$ = 50 hidden size layer. $H_{3}$ = 64 hidden size layer. $H_{4}$ = 32 hidden size layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27129819-0fc9-4528-aece-b3f9524a9ab7",
   "metadata": {},
   "source": [
    "## PROBLEM FORMULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448b2d7-a4f6-48ec-ab54-eee4156d1dd9",
   "metadata": {},
   "source": [
    "The model consists of a group of $N$ agents, represented as $A = [\\mathbf{a}_1, \\ldots, \\mathbf{a}_N]$. In the context of football, there are typically 22 players on the field, so $N=22$. The time-series data we are working with is a sequence of $T$ events denoted as $E = [\\mathbf{e}_1, \\ldots, \\mathbf{e}_T]$. Note that the time-series data is non-uniform, as each element $e_t \\in E$ corresponds to an on-ball action, such as a pass, dribble, or shot, resulting in varying gaps between each time step.\n",
    "\n",
    "For each event $e_t \\in E$, we have a set of observations denoted as $\\Phi_t = [\\boldsymbol{\\phi}{t,1}, \\ldots, \\boldsymbol{\\phi}{t,N}]$, where $\\boldsymbol{\\phi}_{t,n}$ is the observation of agent $n$ at time step $t$. Thus, we obtain a complete set of observations over time denoted as $\\Phi = [\\Phi_1, \\ldots, \\Phi_T]$. However, in our configuration, we only know one value in each $\\Phi_t \\in \\Phi$, meaning that there are $N-1$ missing values for each $\\Phi_t$, and a total of $T(N-1)$ missing values in the entire dataset. This is because we only make observations of the on-the-ball agent at each time step in football, resulting in only one known observation.\n",
    "\n",
    "As the observed agent changes over time, we used a label encoded mask denoted as $M$, where $M_{n,t} = 1$ if agent $n$ is observed at time step $t$, and 0 otherwise. This binary matrix captures all information regarding known and unknown observations across the time-series problem.\n",
    "\n",
    "In this configuration, the objective of the imputation model is to predict values for the unknown observations. Specifically, for each $e_t \\in E$, the model predicts a value $\\hat{\\boldsymbol{\\phi}}_{t,n}$ for every $n \\in [1, \\ldots, N]$, resulting in a complete set of predicted observations denoted as $\\hat{\\Phi} = [\\hat{\\Phi}_1, \\ldots, \\hat{\\Phi}T]$, where $\\hat{\\Phi}t = [\\hat{\\boldsymbol{\\phi}}{t,1}, \\ldots, \\hat{\\boldsymbol{\\phi}}{t,N}]$.\n",
    "\n",
    "In the context of football, the predicted observations $\\hat{\\Phi}$ are the estimated locations of all players (both on-ball and off-ball) for every event $e_t \\in E$. The known observations are derived from the location of the events that occur. For an event $e_t \\in E$ with an on-ball agent $\\mathbf{a}n \\in A$, the assigned observation $\\boldsymbol{\\phi}{t,n}$ is $\\mathbf{e}{t,x,y}$, where $\\mathbf{e}{t,x,y}$ is the $x,y$ position at which $e_t$ occurred.\n",
    "\n",
    "To summarise, the set of known observations $\\Phi$ (containing missing data) is a $(T \\times N \\times 2)$ tensor, and the set of imputed observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a78bf-49be-42c7-8896-175f646f7093",
   "metadata": {},
   "source": [
    "## Model component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516cecf1-504d-468b-91c4-313989bb647b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Time-Aware LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ddc86-3fd5-4868-a601-d83adc9db0d3",
   "metadata": {},
   "source": [
    "The LSTM Component is a crucial part of our approach, where each agent's data is separately passed into a shared bidirectional LSTM as the above figure shows. \n",
    "The input data is divided into N segments of size ($B$ x $L$ x $I$), allowing the LSTM to learn the temporal relationship between the engineered features and agent location. \n",
    "Sharing the LSTM across all agents helps to overcome the issue of sparsity in agent observations by learning common movement patterns for agents with similar roles. \n",
    "To handle the irregular time intervals between timesteps, we used this [Time-Aware LSTM implementation](https://dl.acm.org/doi/10.1145/3097983.3097997). This architecture adjusts cell memory to modify the discount rate of previous or future actions in the sequence based on the difference in time from the current event. This implementation uses an LSTM with a single hidden layer of size $H_{1}$ = 100 .\n",
    "\n",
    "The source code for the **Time-Aware** LSTM can be accessed in the `Time_LSTM.py` file, located in the `model/model architecture` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d833f0b-fe55-400c-af16-9c0ebd66ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class seq_lstm(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size=66, hidden_layer_size=100, output_size=50, batch_size=128\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = TimeLSTM.TimeLSTM(input_size, hidden_layer_size, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_seq, ts):\n",
    "        lstm_out = self.lstm(input_seq, ts)\n",
    "        outs = self.linear(lstm_out[:, -1, :])\n",
    "        outs = self.relu(outs)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fad4f6-3d4e-4530-9f0b-b8f252f27f86",
   "metadata": {},
   "source": [
    "### 2. GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277acfc-c81c-48f3-8821-28d10092ddb9",
   "metadata": {},
   "source": [
    "The LSTM component of the network deals with the temporal behavior of each agent. However, to fully understand the impact of inter-agent relationships within the multi-agent system (MAS), we need to incorporate these relationships into the model. To achieve this, we create a fully connected graph where each node represents an agent and the node features are the temporally aggregated agent representations. With this graph structure, we employ a Graph Neural Network (GNN) to allow information sharing between all agents. The GNN comprises two message passing layers with feature sizes of $H_{3}$ = 64 and $H_{4}$ = 32, respectively, and utilizes the SAGEConv operator. The GNN updates the node features by aggregating information about the agent neighborhoods (i.e., agent interactions) using a mean aggregation scheme.\n",
    "\n",
    "The source code for the **GNN** can be accessed in the `GNN.py` file, located in the `model/model architecture` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2f8f6-4b15-476a-86e7-f188653bdae7",
   "metadata": {},
   "source": [
    "### Loss fuction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d079df-6af7-4b7d-9f71-cfb8694be9be",
   "metadata": {},
   "source": [
    "The loss function measures the average Euclidean distance between the predicted positions and their corresponding ground truth positions.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}\\sqrt{\\sum_{t=1}^{T}(y_{i,t}-\\hat{y}_{i,t})^2}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples, $T$ is the length of the time sequence, $\\hat{y}{i,j}$ is the predicted position of the $i$-th sample at time $j$, and $y{i,j}$ is the true position of the $i$-th sample at time $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6964a4-ef62-4f61-91c2-5f8fc2d81fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def eucl_loss(output, target):\n",
    "    loss = (output - target).pow(2).sum(2).sqrt().mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbba90-0e19-4c82-bc09-0f337311728e",
   "metadata": {},
   "source": [
    "## Agent Imputer Lightning Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dba303-125e-4c60-babe-5a9e58e79fcd",
   "metadata": {},
   "source": [
    "This is a PyTorch Lightning implementation of the model. It has an LSTM component for temporal modeling and a graph convolutional network (GCN) component for modeling the interactions between agents in a multi-agent system. The GCN operates on a fully connected graph with the temporally-aggregated agent representations as node features. The architecture consists of two message passing layers using the SAGEConv operator. The model has a training step, a validation step, and a predict step. The optimizer used is AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc4488-c8d4-4b3e-b04b-4b88eaab0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class AgentImputerLightning(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, input_size=16, hidden_layer_size=100, output_size=2, batch_size=128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstms = seq_lstm(input_size, hidden_layer_size)\n",
    "        self.gcn = GCN.GCN(50)\n",
    "        self.learning_rate = 0.002\n",
    "\n",
    "        # Create edges list\n",
    "        t1 = list(range(22))\n",
    "        list_edges = list(itertools.product(t1, t1))\n",
    "        list_edges = [list(ele) for ele in list_edges if ele[0] != ele[1]]\n",
    "        self.edges = torch.tensor(list_edges).t().contiguous()\n",
    "\n",
    "    def forward(self, input_list, ts_list, edges):\n",
    "        outputs = torch.cat(\n",
    "            [self.lstms(x, ts_l) for x, ts_l in zip(input_list, ts_list)], dim=1\n",
    "        )\n",
    "        outputs = outputs.reshape(outputs.shape[0], 22, 50)\n",
    "        gcn_outputs = self.gcn(outputs, edges)\n",
    "        return gcn_outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_index):\n",
    "        x, y, t = train_batch\n",
    "        input_list = [torch.tensor(x.float())[:, i, :, :] for i in range(0, 22)]\n",
    "        ts_list = [t.float()[:, i, :] for i in range(0, 22)]\n",
    "        y_pred = self.forward(input_list, ts_list, self.edges)\n",
    "        loss = eucl_loss(y_pred, y.float())\n",
    "        self.log(\n",
    "            \"training loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_index):\n",
    "        x, y, t = val_batch\n",
    "        input_list = [torch.tensor(x.float())[:, i, :, :] for i in range(0, 22)]\n",
    "        ts_list = [t.float()[:, i, :] for i in range(0, 22)]\n",
    "        y_pred = self.forward(input_list, ts_list, self.edges)\n",
    "        loss = eucl_loss(y_pred, y.float())\n",
    "        self.log(\n",
    "            \"validation loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y, t = batch\n",
    "        input_list = [torch.tensor(x.float())[:, i, :, :] for i in range(0, 22)]\n",
    "        ts_list = [t.float()[:, i, :] for i in range(0, 22)]\n",
    "        y_pred = self.forward(input_list, ts_list, self.edges)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51286ea7-ba82-4b54-ada9-3f9ac0a04f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ba4ce-f1eb-4316-871d-44d2b5963d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
