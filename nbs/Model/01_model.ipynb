{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6912626-f4cb-4452-8fe2-fd57d5f916bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| default_exp model.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe633eb2-d6d8-438c-97c2-04639d362e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d6e0f-6b5d-4802-8aac-4d77451ed091",
   "metadata": {},
   "source": [
    "## Agent Imputter Model\n",
    "> In this notebook we will load and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ad6ff2-7af1-4cc2-80d2-2551ad032781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateFinder\n",
    "from agent_imputter.model.agent_imputer import AgentImputerLightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de573ae-6cda-48a5-82de-05ec90137af2",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d4a3c-ef37-4463-9e32-5de965a5abd5",
   "metadata": {},
   "source": [
    "### Step 1: Load in pre-saved data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a566fdb5-cc38-44c1-800c-847398769d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# |eval: false\n",
    "\n",
    "# path to games files\n",
    "games_path = Path(\"/home/user/Downloads/Data/games\")\n",
    "match_folder = tuple(\n",
    "    name\n",
    "    for name in os.listdir(games_path)\n",
    "    if os.path.isdir(os.path.join(games_path, name))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fc735-5082-4cf5-80c0-ecde435160e1",
   "metadata": {},
   "source": [
    "### Step 2: Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6aee2-86fb-41d3-8920-7646d485fb0f",
   "metadata": {},
   "source": [
    "In order to handle categorical features in our model, we decided to use label encoding instead of one hot encoding. The main reason is to reduce the dimensionality of the feature space. One hot encoding creates a binary vector for each category, leading to a high-dimensional sparse representation, which can be computationally expensive to handle. Label encoding, on the other hand, assigns a unique integer to each category, resulting in a lower-dimensional representation. By using label encoding, we can reduce the computational complexity of our model while still capturing the categorical information, in our case the number of features has been reduced from $N_{feature}$ = 60 to $N_{feature}$ = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2dc403-1c65-4546-9fe7-9111609eacc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_embedding_tensor_for_game(categories, idx):\n",
    "    \"compute embeddings of categorical features\"\n",
    "\n",
    "    def get_embedding(category_vec, idx):\n",
    "        \"compte embedding of one feature\"\n",
    "        num_classes = len(category_vec.unique())\n",
    "        emb_size = math.floor(math.sqrt(num_classes))\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(category_vec.iloc[idx])\n",
    "        cat = torch.tensor(\n",
    "            np.array(le.transform(category_vec)).reshape(len(category_vec), 1)\n",
    "        ).to(torch.int64)\n",
    "        return cat\n",
    "\n",
    "    cats = torch.tensor([])\n",
    "    for cat in categories:\n",
    "        new_cat = get_embedding(categories[cat], idx)\n",
    "        cats = torch.cat((cats, new_cat), axis=1)\n",
    "    return cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46e08be-2147-4415-b3ee-cdd86c59f175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# emb_tensor = get_embedding_tensor_for_game(features_df[['position','event_type','team_on_ball','player_on_ball','goal_diff']],features_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2c3cd-6a2b-47c6-aa14-a96fedbd3571",
   "metadata": {
    "tags": []
   },
   "source": [
    "We employed the MinMaxScaler method to preprocess the numerical features in our dataset, which involves scaling the data to a specified range, between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a396dc-5488-45e5-ad03-5c6ddd79a39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def preprocess_data(\n",
    "    input_data: pd.DataFrame(),\n",
    ") -> Tuple:\n",
    "    \"\"\"\"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    time_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    emb_cats = get_embedding_tensor_for_game(\n",
    "        input_data[\n",
    "            [\"position\", \"event_type\", \"team_on_ball\", \"player_on_ball\", \"goal_diff\"]\n",
    "        ],\n",
    "        input_data.index,\n",
    "    )\n",
    "    scaler.fit(\n",
    "        input_data[\n",
    "            [\n",
    "                \"ballx\",\n",
    "                \"prev_player_x\",\n",
    "                \"next_player_x\",\n",
    "                \"bally\",\n",
    "                \"prev_player_y\",\n",
    "                \"next_player_y\",\n",
    "                \"av_player_x\",\n",
    "                \"av_player_y\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "    time_scaler.fit(\n",
    "        input_data[[\"time_since_last_pred\", \"prev_player_time\", \"next_player_time\"]]\n",
    "    )\n",
    "\n",
    "    input_data_normalized = scaler.transform(\n",
    "        input_data[\n",
    "            [\n",
    "                \"ballx\",\n",
    "                \"prev_player_x\",\n",
    "                \"next_player_x\",\n",
    "                \"bally\",\n",
    "                \"prev_player_y\",\n",
    "                \"next_player_y\",\n",
    "                \"av_player_x\",\n",
    "                \"av_player_y\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "    input_data_time = time_scaler.transform(\n",
    "        input_data[[\"time_since_last_pred\", \"prev_player_time\", \"next_player_time\"]]\n",
    "    )\n",
    "\n",
    "    input_data_normalized = np.concatenate(\n",
    "        (input_data_normalized, input_data_time), axis=1\n",
    "    )\n",
    "    input_data_normalized = torch.cat(\n",
    "        (torch.tensor(input_data_normalized), emb_cats), 1\n",
    "    )\n",
    "\n",
    "    label_data = torch.tensor(features_df[[\"label_x\", \"label_y\"]].values)\n",
    "    scaler.fit(label_data)\n",
    "    label_data_normalized = scaler.transform(label_data)\n",
    "\n",
    "    return input_data_normalized, label_data_normalized, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af15ece-be21-4f39-9234-22ea433cb0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data_normalized, label_data_normalized, scaler = preprocess_data(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09b9dc-8016-4e26-b75c-18f700479e75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Create seqeuences from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b54f1-21a7-4073-9249-a80e8392dd16",
   "metadata": {},
   "source": [
    "In this section, our aim is to prepare the data in a format suitable for input into our model, which requires a tensor of shape ($N$ × $L$ × $I$), where $N$ is the number of agents, $L$ is the sequence length, and $I$ is the number of features. For our use case, we have 22 players ($N=22$), each with a sequence of 5 events ($L=5$), and 16 features per event ($I=16$), resulting in a tensor of shape ($22$ × $5$ × $16$). Additionally, we need to create a timestamps vector that we will pass to the model, which will also have shape ($22$ × $5$), corresponding to the 22 agents and 5 events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99bfc895-b733-48fa-a7df-fb014bed9503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def split_sequences(\n",
    "    sorted_whole_input_df: pd.DataFrame(),\n",
    "    input_data_normalized: torch.tensor,\n",
    "    label_data_normalized: torch.tensor,\n",
    "    n_steps_in: int,\n",
    "    n_steps_out: int,\n",
    "):\n",
    "    \"Gets sequences of the previous and next x values for input data\"\n",
    "\n",
    "    time_scaler = RobustScaler()\n",
    "    timestamps = torch.tensor(\n",
    "        time_scaler.fit_transform(\n",
    "            np.array(sorted_whole_input_df[\"event_time\"]).reshape(-1, 1)\n",
    "        )\n",
    "    ).reshape(-1)\n",
    "\n",
    "    # Define the number of previous and next tensors to include\n",
    "    num_prev_tensors = n_steps_in\n",
    "    num_next_tensors = n_steps_out\n",
    "\n",
    "    # Create a list to store the resulting tensors\n",
    "    X = []\n",
    "    y = []\n",
    "    ts = []\n",
    "    for i in range(0, int(len(sorted_whole_input_df) / 22)):\n",
    "        prev_indices = range(max(i - num_prev_tensors, 0), i)\n",
    "        next_indices = range(\n",
    "            i + 1, min(i + num_next_tensors + 1, int(len(sorted_whole_input_df) / 22))\n",
    "        )\n",
    "        idx = []\n",
    "\n",
    "        if len(prev_indices) == 0:\n",
    "            idx.extend([i, i, i])\n",
    "        elif len(prev_indices) == 1:\n",
    "            idx.extend([i - 1, i - 1, i])\n",
    "        else:\n",
    "            idx.extend([i - 2, i - 1, i])\n",
    "\n",
    "        if len(next_indices) == 0:\n",
    "            idx.extend([i, i])\n",
    "        elif len(next_indices) == 1:\n",
    "            idx.extend([i + 1, i + 1])\n",
    "        else:\n",
    "            idx.extend([i + 1, i + 2])\n",
    "\n",
    "        l_x = []\n",
    "        l_ts = []\n",
    "        for j in idx:\n",
    "            eve_df = sorted_whole_input_df[sorted_whole_input_df[\"event_num\"] == j]\n",
    "            l_x.append(input_data_normalized[eve_df.index[0] : eve_df.index[-1] + 1])\n",
    "            l_ts.append(timestamps[eve_df.index[0] : eve_df.index[-1] + 1])\n",
    "\n",
    "        # concatenate the tensors along a new dimension\n",
    "        input_result_tensor = torch.stack(l_x, dim=1)\n",
    "        ts_result_tensor = torch.stack(l_ts, dim=1)\n",
    "\n",
    "        # convert to list of 22 tensors of shape (5, 16)\n",
    "        input_tensor_list = [tensor.squeeze(0) for tensor in input_result_tensor]\n",
    "        ts_tensor_list = [tensor.squeeze() for tensor in ts_result_tensor]\n",
    "        ts_tensor_list = [\n",
    "            torch.abs(ts_tensor - ts_tensor[2]) for ts_tensor in ts_tensor_list\n",
    "        ]\n",
    "        X.extend(input_tensor_list)\n",
    "        ts.extend(ts_tensor_list)\n",
    "\n",
    "    y = [tensor.squeeze() for tensor in label_data_normalized]\n",
    "    return X, y, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb0e53d9-d666-4cbc-97b6-bf26f0bdbd67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_ss, y_mm, ts = split_sequences(features_df,input_data_normalized, label_data_normalized, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ef53c-29f6-49fe-a1eb-2aa46a20de10",
   "metadata": {},
   "source": [
    "## Generate and save model inputs for all games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19f78853-6d87-4252-97ed-b2559ece15e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210383_2021-11-20_Leicester City_Chelsea\n",
      "1059830_2019-11-23_Watford_Burnley\n",
      "1059712_2019-08-17_Arsenal_Burnley\n",
      "1059763_2019-09-28_Aston Villa_Burnley\n",
      "2128326_2020-10-17_Chelsea_Southampton\n",
      "1059733_2019-08-31_Burnley_Liverpool\n",
      "1059704_2019-08-10_Burnley_Southampton\n",
      "2210573_2022-04-02_Chelsea_Brentford\n",
      "1059744_2019-09-14_Brighton and Hove Albion_Burnley\n",
      "2210589_2022-04-09_Southampton_Chelsea\n",
      "1059753_2019-09-21_Burnley_Norwich City\n",
      "2210513_2022-04-20_Chelsea_Arsenal\n",
      "2128618_2021-05-01_Chelsea_Fulham\n",
      "2210542_2022-03-05_Burnley_Chelsea\n",
      "2210612_2022-05-01_Everton_Chelsea\n",
      "1059914_2020-01-11_Chelsea_Burnley\n",
      "2128358_2020-11-07_Chelsea_Sheffield United\n",
      "2128294_2020-09-14_Brighton and Hove Albion_Chelsea\n",
      "2128342_2020-10-24_Manchester United_Chelsea\n",
      "2128641_2021-05-12_Chelsea_Arsenal\n",
      "2128413_2020-12-15_Wolverhampton Wanderers_Chelsea\n",
      "1059787_2019-10-19_Leicester City_Burnley\n",
      "2128318_2020-10-03_Chelsea_Crystal Palace\n",
      "1059832_2019-11-30_Burnley_Crystal Palace\n"
     ]
    }
   ],
   "source": [
    "# |eval: false\n",
    "\n",
    "whole_input = pd.DataFrame()\n",
    "l_X_ss = []\n",
    "l_y_mm = []\n",
    "l_ts = []\n",
    "for game in match_folder:\n",
    "    print(game)\n",
    "    features_df = pd.read_csv(Path(f\"{games_path}/{game}/features.csv\"))\n",
    "    whole_input = pd.concat(\n",
    "        [\n",
    "            whole_input,\n",
    "            features_df[\n",
    "                [\"match_id\", \"event_id\", \"player_id\", \"position\", \"label_x\", \"label_y\"]\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "    input_data_normalized, label_data_normalized, scaler = preprocess_data(features_df)\n",
    "    X_ss, y_mm, ts = split_sequences(\n",
    "        features_df, input_data_normalized, label_data_normalized, 2, 2\n",
    "    )\n",
    "    l_X_ss = [*l_X_ss, *X_ss]\n",
    "    l_y_mm = [*l_y_mm, *y_mm]\n",
    "    l_ts = [*l_ts, *ts]\n",
    "\n",
    "l_event_id = whole_input[\"event_id\"].values\n",
    "l_match_id = whole_input[\"match_id\"].values\n",
    "l_player_id = whole_input[\"player_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82d9c008-cf01-4093-ae19-fd1477676e56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save input df\n",
    "# whole_input.to_csv(\"/home/user/Downloads/agent_imputter_output/17_pred/whole_input.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05437919-acae-4f27-8fdf-20164bd1a951",
   "metadata": {},
   "source": [
    "## Split train and test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139f2ca-1e2d-4508-9c18-e6a74ebaa740",
   "metadata": {},
   "source": [
    "In this section, we will split the dataset into two parts: the training set and the test set. To ensure consistency with the paper, we will use the same proportion of `91.2%` for the training set and `8.8%` for the test set. This division of the data will allow us to train the model on a sufficiently large sample while also ensuring that we have enough data left over to evaluate its performance on unseen data. By following this approach, we can ensure that our results are comparable to those reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2648d3b4-39fa-4d4a-8a22-1e61e0c3775d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_train_test_split(\n",
    "    X_ss, y_mm, ts, l_event_id, l_match_id, l_player_id, train_size=0.912, shuffle=False\n",
    "):\n",
    "    \"Get train and test split\"\n",
    "    train_size = int((len(X_ss) / 22) * train_size) * 22\n",
    "    (\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        ts_train,\n",
    "        ts_test,\n",
    "        event_ids_train,\n",
    "        event_ids_test,\n",
    "        match_ids_train,\n",
    "        match_ids_test,\n",
    "        player_ids_train,\n",
    "        player_ids_test,\n",
    "    ) = train_test_split(\n",
    "        X_ss,\n",
    "        y_mm,\n",
    "        ts,\n",
    "        l_event_id,\n",
    "        l_match_id,\n",
    "        l_player_id,\n",
    "        random_state=42,\n",
    "        shuffle=shuffle,\n",
    "        train_size=train_size,\n",
    "    )\n",
    "\n",
    "    events_ids = [event_ids_train, event_ids_test]\n",
    "    match_ids = [match_ids_train, match_ids_test]\n",
    "    player_ids = [player_ids_train, player_ids_test]\n",
    "    return (\n",
    "        tuple(X_train),\n",
    "        tuple(X_test),\n",
    "        torch.tensor(np.array(y_train)),\n",
    "        torch.tensor(np.array(y_test)),\n",
    "        tuple(ts_train),\n",
    "        tuple(ts_test),\n",
    "        events_ids,\n",
    "        match_ids,\n",
    "        player_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bca052d-c1df-4230-bcf7-6d0da2dfde4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    X_train_ts,\n",
    "    X_test_ts,\n",
    "    events_ids,\n",
    "    match_ids,\n",
    "    player_ids,\n",
    ") = get_train_test_split(l_X_ss, l_y_mm, l_ts, l_event_id, l_match_id, l_player_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a739422-482a-46c8-814c-b3d6452a7a67",
   "metadata": {},
   "source": [
    "### Convert to pytorch Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f88a81-68e4-4964-853d-4a374b17b002",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section we will convert our dataset to pytorch dataset and then to pytorch dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f554091-3418-437f-b3ae-d3e9b5de660b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class series_data(Dataset):\n",
    "    \"Convert into chunks of 22 players and sequences of 5 with all features\"\n",
    "\n",
    "    def __init__(self, x, y, t, feature_num):\n",
    "        self.x = torch.stack(x).reshape(int(len(x) / 22), 22, 5, feature_num)\n",
    "        self.y = y.clone().detach().float().reshape(int(len(x) / 22), 22, 2)\n",
    "        self.t = torch.stack(t).reshape(int(len(x) / 22), 22, 5)\n",
    "        self.len = int(len(x) / 22)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx], self.t[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5082ae95-ee39-425d-b0ac-e05da51e8480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same batch size used in paper.\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39514b35-5d38-4b92-a19b-20ba89f6ea38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put into format of 22 event sequences for an event (representing each player) and put into data loaders\n",
    "train_data = series_data(X_train, y_train, X_train_ts, 16)\n",
    "test_data = series_data(X_test, y_test, X_test_ts, 16)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, shuffle=False, batch_size=BATCH_SIZE, num_workers=12\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data, shuffle=False, batch_size=BATCH_SIZE, num_workers=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b01bd7-300d-4891-b711-19a49c69853c",
   "metadata": {},
   "source": [
    "## Load and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6383a7d1-34ea-4a86-965c-b47632afa7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mislamb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230418_231412-gbwr50hg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/islamb/agent_imputter/runs/gbwr50hg' target=\"_blank\">twilight-sun-16</a></strong> to <a href='https://wandb.ai/islamb/agent_imputter' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/islamb/agent_imputter' target=\"_blank\">https://wandb.ai/islamb/agent_imputter</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/islamb/agent_imputter/runs/gbwr50hg' target=\"_blank\">https://wandb.ai/islamb/agent_imputter/runs/gbwr50hg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# |eval: false\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"agent_imputter\")\n",
    "wandb_logger.experiment.config[\"batch_size\"] = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74112327-f50a-4157-bddc-93ab2e38494a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initiate model callbacks\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"validation loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ee27780-afe1-4d6c-b14b-ace7ef738971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initiate model\n",
    "model = AgentImputerLightning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1f87b3e-b960-47be-b763-c478d3dac481",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# initiate trainer\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, LearningRateFinder(0.01, 0.001)],\n",
    "    max_epochs=3,\n",
    "    min_epochs=1,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=wandb_logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc4691e5-985d-45bd-a9ed-ee8bdfa3d6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/agent_imputter/lib/python3.9/site-packages/agent_imputter/model/agent_imputer.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_list = [torch.tensor(x.float())[:,i,:,:] for i in range(0,22)]\n",
      "Finding best initial lr:   0%|                          | 0/100 [00:00<?, ?it/s]/home/user/anaconda3/envs/agent_imputter/lib/python3.9/site-packages/agent_imputter/model/agent_imputer.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_list = [torch.tensor(x.float())[:,i,:,:] for i in range(0,22)]\n",
      "Finding best initial lr: 100%|████████████████| 100/100 [00:12<00:00,  8.31it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|████████████████| 100/100 [00:12<00:00,  8.04it/s]\n",
      "Learning rate set to 0.007762471166286917\n",
      "Restoring states from the checkpoint path at /home/user/Git_repos/agent_imputter/nbs/Model/.lr_find_161868dc-7cfc-47a1-bd2c-b9998c5480dd.ckpt\n",
      "Restored all states from the checkpoint at /home/user/Git_repos/agent_imputter/nbs/Model/.lr_find_161868dc-7cfc-47a1-bd2c-b9998c5480dd.ckpt\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | lstms | seq_lstm | 62.4 K\n",
      "1 | gcn   | GCN      | 10.7 K\n",
      "-----------------------------------\n",
      "73.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "73.0 K    Total params\n",
      "0.292     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at /home/user/Git_repos/agent_imputter/nbs/Model/.lr_find_161868dc-7cfc-47a1-bd2c-b9998c5480dd.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████| 270/270 [00:36<00:00,  7.49it/s, v_num=50hg]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 1/27 [00:00<00:02, 12.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 2/27 [00:00<00:01, 14.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 3/27 [00:00<00:01, 15.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 4/27 [00:00<00:01, 16.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▌               | 5/27 [00:00<00:01, 16.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▏              | 6/27 [00:00<00:01, 16.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▉              | 7/27 [00:00<00:01, 16.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▋             | 8/27 [00:00<00:01, 17.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 9/27 [00:00<00:01, 17.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 10/27 [00:00<00:00, 17.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 11/27 [00:00<00:00, 17.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 12/27 [00:00<00:00, 17.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 13/27 [00:00<00:00, 17.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 14/27 [00:00<00:00, 17.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 15/27 [00:00<00:00, 17.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 16/27 [00:00<00:00, 17.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 17/27 [00:00<00:00, 17.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 18/27 [00:01<00:00, 17.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 19/27 [00:01<00:00, 17.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 20/27 [00:01<00:00, 17.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 21/27 [00:01<00:00, 17.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 22/27 [00:01<00:00, 17.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 23/27 [00:01<00:00, 17.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 24/27 [00:01<00:00, 17.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 25/27 [00:01<00:00, 17.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 26/27 [00:01<00:00, 17.42it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 270/270 [00:38<00:00,  6.97it/s, v_num=50hg, validation loss=0.\u001b[A\n",
      "Epoch 1: 100%|█| 270/270 [00:39<00:00,  6.80it/s, v_num=50hg, validation loss=0.\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 1/27 [00:00<00:02, 12.27it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 2/27 [00:00<00:01, 13.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 3/27 [00:00<00:01, 14.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 4/27 [00:00<00:01, 14.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▌               | 5/27 [00:00<00:01, 15.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▏              | 6/27 [00:00<00:01, 15.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▉              | 7/27 [00:00<00:01, 15.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▋             | 8/27 [00:00<00:01, 15.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 9/27 [00:00<00:01, 15.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 10/27 [00:00<00:01, 15.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 11/27 [00:00<00:01, 14.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 12/27 [00:00<00:00, 15.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 13/27 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 14/27 [00:00<00:00, 15.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 15/27 [00:00<00:00, 15.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 16/27 [00:01<00:00, 15.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 17/27 [00:01<00:00, 15.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 18/27 [00:01<00:00, 15.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 19/27 [00:01<00:00, 15.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 20/27 [00:01<00:00, 16.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 21/27 [00:01<00:00, 16.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 22/27 [00:01<00:00, 16.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 23/27 [00:01<00:00, 16.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 24/27 [00:01<00:00, 16.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 25/27 [00:01<00:00, 16.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 26/27 [00:01<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 270/270 [00:42<00:00,  6.42it/s, v_num=50hg, validation loss=0.\u001b[A\n",
      "Epoch 2: 100%|█| 270/270 [00:39<00:00,  6.91it/s, v_num=50hg, validation loss=0.\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 1/27 [00:00<00:02, 12.22it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 2/27 [00:00<00:01, 14.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 3/27 [00:00<00:01, 15.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 4/27 [00:00<00:01, 15.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▌               | 5/27 [00:00<00:01, 15.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▏              | 6/27 [00:00<00:01, 16.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▉              | 7/27 [00:00<00:01, 16.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▋             | 8/27 [00:00<00:01, 16.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 9/27 [00:00<00:01, 16.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 10/27 [00:00<00:01, 16.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 11/27 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 12/27 [00:00<00:00, 16.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 13/27 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 14/27 [00:00<00:00, 16.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 15/27 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 16/27 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 17/27 [00:01<00:00, 16.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 18/27 [00:01<00:00, 16.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 19/27 [00:01<00:00, 16.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 20/27 [00:01<00:00, 16.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 21/27 [00:01<00:00, 16.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 22/27 [00:01<00:00, 16.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 23/27 [00:01<00:00, 16.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 24/27 [00:01<00:00, 16.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 25/27 [00:01<00:00, 16.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 26/27 [00:01<00:00, 16.67it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 270/270 [00:41<00:00,  6.52it/s, v_num=50hg, validation loss=0.\u001b[A\n",
      "Epoch 2: 100%|█| 270/270 [00:41<00:00,  6.52it/s, v_num=50hg, validation loss=0.\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█| 270/270 [00:41<00:00,  6.52it/s, v_num=50hg, validation loss=0.\n"
     ]
    }
   ],
   "source": [
    "# |eval: false\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ae84d69-ee31-4893-8dd9-820a1a137115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |eval: false\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d3ad8-82fb-4b6a-b311-7119028bdd54",
   "metadata": {},
   "source": [
    "## predict and save train and test prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c71c28-2bc4-44da-a0ba-fefe1bb41e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# |eval: false\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=len(train_data) * 22)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=len(test_data) * 22)\n",
    "\n",
    "_train_p = trainer.predict(model, train_loader)\n",
    "_test_p = trainer.predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f984de-6278-4639-8ffe-58b5bc80121e",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cd0ca-75cd-462f-a965-e1a24ddfb147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# |eval: false\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df[\"match_id\"], train_df[\"event_id\"], train_df[\"player_id\"] = (\n",
    "    match_ids[0],\n",
    "    events_ids[0],\n",
    "    player_ids[0],\n",
    ")\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df[\"match_id\"], test_df[\"event_id\"], test_df[\"player_id\"] = (\n",
    "    match_ids[1],\n",
    "    events_ids[1],\n",
    "    player_ids[1],\n",
    ")\n",
    "\n",
    "train_p = scaler.inverse_transform(_train_p[0].reshape(len(train_data) * 22, 2))\n",
    "test_p = scaler.inverse_transform(_test_p[0].reshape(len(test_data) * 22, 2))\n",
    "train_y = scaler.inverse_transform(y_train)\n",
    "test_y = scaler.inverse_transform(y_test)\n",
    "\n",
    "train_df[\"pred_x\"], train_df[\"pred_y\"] = train_p[:, 0], train_p[:, 1]\n",
    "train_df[\"act_x\"], train_df[\"act_y\"] = train_y[:, 0], train_y[:, 1]\n",
    "\n",
    "test_df[\"pred_x\"], test_df[\"pred_y\"] = test_p[:, 0], test_p[:, 1]\n",
    "test_df[\"act_x\"], test_df[\"act_y\"] = test_y[:, 0], test_y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae06bc1-5faf-4db3-985a-5b6b2f683563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# |eval: false\n",
    "\n",
    "# train_df.to_csv(\"/home/user/Downloads/agent_imputter_output/17_pred/train_df.csv\")\n",
    "# test_df.to_csv(\"/home/user/Downloads/agent_imputter_output/17_pred/test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f205e-2f8c-4e60-ae63-a0a2206f9245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defdadf-6249-47bb-a6d2-9824fe3dee78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df353a5-d937-40a6-b0c5-0f06328aa45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
