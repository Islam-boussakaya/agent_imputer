# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/Model/model_architecture/Agent_Imputer.ipynb.

# %% auto 0
__all__ = ['seq_lstm', 'eucl_loss', 'AgentImputerLightning']

# %% ../../nbs/Model/model_architecture/Agent_Imputer.ipynb 2
import torch
from torch import nn
import model_architecture.Time_LSTM_Module as TimeLSTM
import model_architecture.GNN_Module as GCN
import pytorch_lightning as pl
import itertools

# %% ../../nbs/Model/model_architecture/Agent_Imputer.ipynb 11
class seq_lstm(nn.Module):
    def __init__(
        self, input_size=66, hidden_layer_size=100, output_size=50, batch_size=128
    ):
        super().__init__()

        self.hidden_layer_size = hidden_layer_size
        self.lstm = TimeLSTM.TimeLSTM(input_size, hidden_layer_size, bidirectional=True)
        self.linear = nn.Linear(hidden_layer_size, output_size)
        self.batch_size = batch_size
        self.relu = nn.ReLU()

    def forward(self, input_seq, ts):
        lstm_out = self.lstm(input_seq, ts)
        outs = self.linear(lstm_out[:, -1, :])
        outs = self.relu(outs)
        return outs

# %% ../../nbs/Model/model_architecture/Agent_Imputer.ipynb 16
def eucl_loss(output, target):
    loss = (output - target).pow(2).sum(2).sqrt().mean()
    return loss

# %% ../../nbs/Model/model_architecture/Agent_Imputer.ipynb 19
class AgentImputerLightning(pl.LightningModule):
    def __init__(
        self, input_size=16, hidden_layer_size=100, output_size=2, batch_size=128
    ):
        super().__init__()
        self.hidden_layer_size = hidden_layer_size
        self.lstms = seq_lstm(input_size, hidden_layer_size)
        self.gcn = GCN.GCN(50)
        self.learning_rate = 0.002

        # Create edges list
        t1 = list(range(22))
        list_edges = list(itertools.product(t1, t1))
        list_edges = [list(ele) for ele in list_edges if ele[0] != ele[1]]
        self.edges = torch.tensor(list_edges).t().contiguous()

    def forward(self, input_list, ts_list, edges):
        outputs = torch.cat(
            [self.lstms(x, ts_l) for x, ts_l in zip(input_list, ts_list)], dim=1
        )
        outputs = outputs.reshape(outputs.shape[0], 22, 50)
        gcn_outputs = self.gcn(outputs, edges)
        return gcn_outputs

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)
        return optimizer

    def training_step(self, train_batch, batch_index):
        x, y, t = train_batch
        input_list = [torch.tensor(x.float())[:, i, :, :] for i in range(0, 22)]
        ts_list = [t.float()[:, i, :] for i in range(0, 22)]
        y_pred = self.forward(input_list, ts_list, self.edges)
        loss = eucl_loss(y_pred, y.float())
        self.log(
            "training loss",
            loss,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )
        return loss

    def validation_step(self, val_batch, batch_index):
        x, y, t = val_batch
        input_list = [torch.tensor(x.float())[:, i, :, :] for i in range(0, 22)]
        ts_list = [t.float()[:, i, :] for i in range(0, 22)]
        y_pred = self.forward(input_list, ts_list, self.edges)
        loss = eucl_loss(y_pred, y.float())
        self.log(
            "validation loss",
            loss,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        x, y, t = batch
        input_list = [torch.tensor(x.float())[:, i, :, :] for i in range(0, 22)]
        ts_list = [t.float()[:, i, :] for i in range(0, 22)]
        y_pred = self.forward(input_list, ts_list, self.edges)
        return y_pred
